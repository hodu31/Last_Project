{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(col_0   0   1   2\n",
       " row_0            \n",
       " 0      19   0   0\n",
       " 1       0  13   0\n",
       " 2       0   0  13,\n",
       " 0.431374165886554)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터셋 로드\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_target = iris.target\n",
    "\n",
    "# 데이터셋을 train과 test로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_df, iris_target, test_size=0.3, random_state=42)\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 최적의 클러스터 수 찾기 (엘보우 방법)\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "K = range(2, 11)  # 클러스터의 수를 2부터 10까지 시험해본다.\n",
    "for k in K:\n",
    "    kmeans_model = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans_model.fit(X_train_scaled)\n",
    "    inertia.append(kmeans_model.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_train_scaled, kmeans_model.labels_))\n",
    "\n",
    "# 최적의 클러스터 수를 선택하는 것은 실루엣 점수가 높고 관성이 낮은 지점을 찾는 것을 포함한다.\n",
    "# 여기서는 이 단계를 단순화하고 예제의 목적상 n_clusters=3을 사용하겠다.\n",
    "\n",
    "# KMeans 모델 생성 및 학습\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_train_scaled)\n",
    "\n",
    "# 테스트 데이터셋에 대한 클러스터 예측\n",
    "predicted_labels = kmeans.predict(X_test_scaled)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 다시 매핑 함수를 사용하여 예측된 클러스터 레이블을 실제 레이블에 매핑\n",
    "def map_clusters_to_labels(predicted_labels, true_labels):\n",
    "    mapped_labels = np.zeros_like(predicted_labels)\n",
    "    for i in range(kmeans.n_clusters):\n",
    "        mask = (predicted_labels == i)\n",
    "        mapped_labels[mask] = np.bincount(true_labels[mask]).argmax()\n",
    "    return mapped_labels\n",
    "\n",
    "mapped_labels = map_clusters_to_labels(predicted_labels, y_test)\n",
    "\n",
    "# 매핑된 레이블로 새로운 혼동 행렬 생성\n",
    "new_misclassification_table = pd.crosstab(y_test, mapped_labels, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "new_misclassification_table\n",
    "\n",
    "# 실루엣 점수 계산\n",
    "silhouette_avg = silhouette_score(X_test_scaled, predicted_labels)\n",
    "\n",
    "misclassification_table, silhouette_avg, #inertia, #silhouette_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서포트 벡터 머신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9809384572542468,\n",
       " 0.9809523809523809,\n",
       " col_0   0   1   2\n",
       " row_0            \n",
       " 0      19   0   0\n",
       " 1       0  13   0\n",
       " 2       0   0  13,\n",
       " 0.9809523809523809,\n",
       " 0.9819291819291819)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# iris 데이터셋 로드\n",
    "iris = load_iris()\n",
    "\n",
    "# iris 데이터프레임 생성\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# iris target 값 설정\n",
    "iris_target = iris.target\n",
    "\n",
    "# train set과 test set으로 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_df, iris_target, test_size=0.3, random_state=42)\n",
    "\n",
    "# SVM 모델 생성\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "# 랜덤 서치를 위한 매개변수 분포 설정\n",
    "param_dist = {'C': uniform(loc=0.1, scale=9.9), 'kernel': ['linear', 'rbf', 'poly'], 'gamma': ['scale', 'auto']}\n",
    "\n",
    "# 교차 검증을 포함한 랜덤 서치 수행\n",
    "random_search = RandomizedSearchCV(svm, param_dist, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 랜덤 서치에서 최적의 모델 선택\n",
    "best_svm = random_search.best_estimator_\n",
    "\n",
    "# 테스트 데이터로 예측\n",
    "predicted_labels = best_svm.predict(X_test)\n",
    "\n",
    "# 오분류 테이블 생성\n",
    "misclassification_table = pd.crosstab(y_test, predicted_labels)\n",
    "\n",
    "# 가중치 평균 F1 스코어 계산\n",
    "f1 = f1_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "# 재현율 계산\n",
    "recall = recall_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 정밀도 계산\n",
    "precision = precision_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 오분류 테이블, F1 스코어, 정확도, 재현율, 정밀도 출력\n",
    "misclassification_table, f1, accuracy, recall, precision\n",
    "\n",
    "# 과적합 확인을 위해 훈련 데이터로 예측\n",
    "train_predicted_labels = best_svm.predict(X_train)\n",
    "\n",
    "# 훈련 데이터의 F1 스코어 계산\n",
    "train_f1 = f1_score(y_train, train_predicted_labels, average='weighted')\n",
    "\n",
    "# 훈련 데이터의 정확도 계산\n",
    "train_accuracy = accuracy_score(y_train, train_predicted_labels)\n",
    "\n",
    "# 훈련 데이터의 재현율 계산\n",
    "train_recall = recall_score(y_train, train_predicted_labels, average='weighted')\n",
    "\n",
    "# 훈련 데이터의 정밀도 계산\n",
    "train_precision = precision_score(y_train, train_predicted_labels, average='weighted')\n",
    "\n",
    "# 훈련 데이터의 F1 스코어, 정확도, 오분류 테이블, 재현율, 정밀도 출력\n",
    "train_f1, train_accuracy, misclassification_table, train_recall, train_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의사결정 나무"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9713815359690611,\n",
       " 0.9714285714285714,\n",
       " col_0   0   1   2\n",
       " row_0            \n",
       " 0      19   0   0\n",
       " 1       0  13   0\n",
       " 2       0   0  13,\n",
       " 0.9714285714285714,\n",
       " 0.9735714285714285,\n",
       " {'min_samples_split': 4, 'max_depth': 4})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "# iris 데이터셋 로드\n",
    "iris = load_iris()\n",
    "\n",
    "# iris 데이터를 데이터프레임으로 변환\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# iris target 값 저장\n",
    "iris_target = iris.target\n",
    "\n",
    "# train set과 test set으로 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_df, iris_target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision Tree 분류기 객체 생성\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Random Search를 위한 매개변수 그리드 정의\n",
    "param_grid = {'max_depth': [2, 3, 4, 5], 'min_samples_split': [2, 3, 4, 5]}\n",
    "\n",
    "# Random Search 수행\n",
    "random_search = RandomizedSearchCV(dt, param_distributions=param_grid, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Random Search로부터 최적의 분류기 객체 추출\n",
    "dt_best = random_search.best_estimator_\n",
    "\n",
    "# 테스트 데이터에 대한 예측값 생성\n",
    "predicted_labels = dt_best.predict(X_test)\n",
    "\n",
    "# 오분류 테이블 생성\n",
    "misclassification_table = pd.crosstab(y_test, predicted_labels)\n",
    "\n",
    "# 가중치 평균 F1 스코어 계산\n",
    "f1 = f1_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "# 재현율 계산\n",
    "recall = recall_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 정밀도 계산\n",
    "precision = precision_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 오분류 테이블, F1 스코어, 정확도, 재현율, 정밀도 출력\n",
    "misclassification_table, f1, accuracy, recall, precision\n",
    "\n",
    "# 과적합 확인을 위한 학습 데이터에 대한 예측값 생성\n",
    "train_predicted_labels = dt_best.predict(X_train)\n",
    "\n",
    "# 학습 데이터에 대한 가중치 평균 F1 스코어 계산\n",
    "train_f1 = f1_score(y_train, train_predicted_labels, average='weighted')\n",
    "\n",
    "# 학습 데이터에 대한 정확도 계산\n",
    "train_accuracy = accuracy_score(y_train, train_predicted_labels)\n",
    "\n",
    "# 학습 데이터에 대한 재현율 계산\n",
    "train_recall = recall_score(y_train, train_predicted_labels, average='weighted')\n",
    "\n",
    "# 학습 데이터에 대한 정밀도 계산\n",
    "train_precision = precision_score(y_train, train_predicted_labels, average='weighted')\n",
    "\n",
    "# 학습 데이터에 대한 F1 스코어, 정확도, 오분류 테이블, 재현율, 정밀도 출력\n",
    "train_f1, train_accuracy, misclassification_table, train_recall, train_precision, random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "10 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\de31\\anaconda3\\envs\\lp_gpu\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.92380952 0.62857143 0.93333333 0.91428571 0.93333333\n",
      "        nan 0.91428571 0.62857143 0.91428571]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9523025599484354,\n",
       " 0.9523809523809523,\n",
       " col_0   0   1   2\n",
       " row_0            \n",
       " 0      19   0   0\n",
       " 1       0  13   0\n",
       " 2       0   0  13,\n",
       " 0.9523809523809523,\n",
       " 0.9543977591036416,\n",
       " {'min_samples_split': 2, 'max_depth': 3})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "# iris 데이터셋 로드\n",
    "iris = load_iris()\n",
    "\n",
    "# iris 데이터를 데이터프레임으로 변환\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# iris target 값 저장\n",
    "iris_target = iris.target\n",
    "\n",
    "# train set과 test set으로 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_df, iris_target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision Tree 분류기 객체 생성\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Random Search를 위한 매개변수 그리드 정의\n",
    "param_grid = {'max_depth': [1,2,3,4,5], 'min_samples_split': [1, 2,3,4,5]}\n",
    "\n",
    "# Random Search 수행\n",
    "random_search = RandomizedSearchCV(dt, param_distributions=param_grid, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Random Search로부터 최적의 분류기 객체 추출\n",
    "dt_best = random_search.best_estimator_\n",
    "\n",
    "# 테스트 데이터에 대한 예측값 생성\n",
    "predicted_labels = dt_best.predict(X_test)\n",
    "\n",
    "# 오분류 테이블 생성\n",
    "misclassification_table = pd.crosstab(y_test, predicted_labels)\n",
    "\n",
    "# 가중치 평균 F1 스코어 계산\n",
    "f1 = f1_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "# 재현율 계산\n",
    "recall = recall_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 정밀도 계산\n",
    "precision = precision_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 오분류 테이블, F1 스코어, 정확도, 재현율, 정밀도 출력\n",
    "misclassification_table, f1, accuracy, recall, precision\n",
    "\n",
    "# 과적합 확인을 위한 학습 데이터에 대한 예측값 생성\n",
    "train_predicted_labels = dt_best.predict(X_train)\n",
    "\n",
    "# 학습 데이터에 대한 가중치 평균 F1 스코어 계산\n",
    "train_f1 = f1_score(y_train, train_predicted_labels, average='weighted')\n",
    "\n",
    "# 학습 데이터에 대한 정확도 계산\n",
    "train_accuracy = accuracy_score(y_train, train_predicted_labels)\n",
    "\n",
    "# 학습 데이터에 대한 재현율 계산\n",
    "train_recall = recall_score(y_train, train_predicted_labels, average='weighted')\n",
    "\n",
    "# 학습 데이터에 대한 정밀도 계산\n",
    "train_precision = precision_score(y_train, train_predicted_labels, average='weighted')\n",
    "\n",
    "# 학습 데이터에 대한 F1 스코어, 정확도, 오분류 테이블, 재현율, 정밀도 출력\n",
    "train_f1, train_accuracy, misclassification_table, train_recall, train_precision, random_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜덤 포레스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_estimators': 100,\n",
       "  'min_samples_split': 5,\n",
       "  'min_samples_leaf': 1,\n",
       "  'max_depth': None},\n",
       " Predicted   0   1   2\n",
       " Actual               \n",
       " 0          19   0   0\n",
       " 1           0  13   0\n",
       " 2           0   0  13,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9809384572542468,\n",
       " 0.9809523809523809,\n",
       " 0.9819291819291819,\n",
       " 0.9809523809523809)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# 코드 실행을 위한 필요한 라이브러리 임포트\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "# Iris 데이터셋 로드\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_target = iris.target\n",
    "\n",
    "# 데이터를 학습 세트와 테스트 세트로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_df, iris_target, test_size=0.3, random_state=42)\n",
    "\n",
    "# 랜덤 포레스트 분류기 초기화\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 하이퍼파라미터 그리드 설정\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# 랜덤 검색 객체 초기화\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 파라미터와 모델 찾기\n",
    "best_params = random_search.best_params_\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# 테스트 데이터에 대한 예측 및 평가 지표 계산\n",
    "predicted_labels = best_rf.predict(X_test)\n",
    "misclassification_table = pd.crosstab(y_test, predicted_labels, rownames=['Actual'], colnames=['Predicted'])\n",
    "f1_test = f1_score(y_test, predicted_labels, average='weighted')\n",
    "accuracy_test = accuracy_score(y_test, predicted_labels)\n",
    "precision_test = precision_score(y_test, predicted_labels, average='weighted')\n",
    "recall_test = recall_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 학습 데이터에 대한 예측 및 평가 지표 계산\n",
    "train_predicted_labels = best_rf.predict(X_train)\n",
    "train_f1 = f1_score(y_train, train_predicted_labels, average='weighted')\n",
    "train_accuracy = accuracy_score(y_train, train_predicted_labels)\n",
    "precision_train = precision_score(y_train, train_predicted_labels, average='weighted')\n",
    "recall_train = recall_score(y_train, train_predicted_labels, average='weighted')\n",
    "\n",
    "# 결과 출력\n",
    "(best_params, misclassification_table, f1_test, accuracy_test, precision_test, recall_test,\n",
    " train_f1, train_accuracy, precision_train, recall_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그레디언트 부스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " 1.0,\n",
       " col_0   0   1   2\n",
       " row_0            \n",
       " 0      19   0   0\n",
       " 1       0  13   0\n",
       " 2       0   0  13,\n",
       " 1.0,\n",
       " 1.0)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# iris 데이터셋 로드\n",
    "iris = load_iris()\n",
    "\n",
    "# iris 데이터프레임 생성\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# iris target 값 설정\n",
    "iris_target = iris.target\n",
    "\n",
    "# train 데이터와 test 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_df, iris_target, test_size=0.3, random_state=42)\n",
    "\n",
    "# GradientBoostingClassifier 모델 생성\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# 탐색할 하이퍼파라미터 범위 설정\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 201),\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# 랜덤 서치를 통한 최적의 하이퍼파라미터 탐색\n",
    "random_search = RandomizedSearchCV(gb, param_distributions=param_dist, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 모델 선택\n",
    "best_gb = random_search.best_estimator_\n",
    "\n",
    "# 테스트 데이터로 예측\n",
    "predicted_labels = best_gb.predict(X_test)\n",
    "\n",
    "# 오분류 테이블 생성\n",
    "misclassification_table = pd.crosstab(y_test, predicted_labels)\n",
    "\n",
    "# F1 스코어 계산\n",
    "f1 = f1_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "# 오분류 테이블, F1 스코어, 정확도 출력\n",
    "misclassification_table, f1, accuracy\n",
    "\n",
    "# 과적합 확인을 위해 훈련 데이터로 예측\n",
    "train_predicted_labels = best_gb.predict(X_train)\n",
    "\n",
    "# 훈련 데이터의 F1 스코어 계산\n",
    "train_f1 = f1_score(y_train, train_predicted_labels, average='weighted')\n",
    "\n",
    "# 훈련 데이터의 정확도 계산\n",
    "train_accuracy = accuracy_score(y_train, train_predicted_labels)\n",
    "\n",
    "# 재현율 계산\n",
    "recall = recall_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 정밀도 계산\n",
    "precision = precision_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 훈련 데이터의 F1 스코어, 정확도,재현율, 정밀도, 오분류 테이블 출력\n",
    "train_f1, train_accuracy, misclassification_table, recall, precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 히스토그램 그레디언트 부스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " 1.0,\n",
       " col_0   0   1   2\n",
       " row_0            \n",
       " 0      19   0   0\n",
       " 1       0  11   2\n",
       " 2       0   0  13,\n",
       " 0.9555555555555556,\n",
       " 0.9614814814814815)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# iris 데이터셋 로드\n",
    "iris = load_iris()\n",
    "\n",
    "# iris 데이터를 DataFrame으로 변환\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# iris target 값\n",
    "iris_target = iris.target\n",
    "\n",
    "# train set과 test set으로 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_df, iris_target, test_size=0.3, random_state=42)\n",
    "\n",
    "# HistGradientBoostingClassifier 모델 생성\n",
    "gb = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter 탐색을 위한 그리드 설정\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'max_leaf_nodes': [None, 5, 10],\n",
    "    'min_samples_leaf': [1, 3, 5],\n",
    "    'l2_regularization': [0.0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# 랜덤 서치를 통한 Hyperparameter 탐색\n",
    "random_search = RandomizedSearchCV(gb, param_distributions=param_grid, n_iter=10, cv=5, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 모델 선택\n",
    "gb_best = random_search.best_estimator_\n",
    "\n",
    "# 테스트 데이터에 대한 예측\n",
    "predicted_labels = gb_best.predict(X_test)\n",
    "\n",
    "# 오분류 테이블 생성\n",
    "misclassification_table = pd.crosstab(y_test, predicted_labels)\n",
    "\n",
    "# F1 스코어 계산\n",
    "f1 = f1_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "\n",
    "# 오분류 테이블, F1 스코어, 정확도 출력\n",
    "misclassification_table, f1, accuracy\n",
    "\n",
    "# 과적합 확인을 위해 학습 데이터에 대한 예측\n",
    "train_predicted_labels = gb_best.predict(X_train)\n",
    "\n",
    "# 학습 데이터에 대한 F1 스코어 계산\n",
    "train_f1 = f1_score(y_train, train_predicted_labels, average='weighted')\n",
    "\n",
    "# 학습 데이터에 대한 정확도 계산\n",
    "train_accuracy = accuracy_score(y_train, train_predicted_labels)\n",
    "\n",
    "# 재현율과 정밀도 계산\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "\n",
    "# 테스트 데이터에 대한 재현율 계산\n",
    "recall = recall_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "# 테스트 데이터에 대한 정밀도 계산\n",
    "precision = precision_score(y_test, predicted_labels, average='weighted')\n",
    "\n",
    "\n",
    "# 학습 데이터에 대한 F1 스코어, 정확도, 오분류 테이블 출력\n",
    "train_f1, train_accuracy, misclassification_table, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lp_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
